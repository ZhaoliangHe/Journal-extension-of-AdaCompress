\section{Related Works}
\label{Section: related_works}

As online computer vision-based services have become the norm for today's applications~\cite{huynh2017deepmon, agrawal2015cloudcv}, many studies have been devoted to improving the cloud-based model execution, including model compression and data compression.

\subsection{Model Compression}

Though the accurate term is still for the community to debate, we use ``model compression'' to represent the studies on \emph{compressing} and \emph{moving} the deep learning models close to users. Many studies tried to compress the deep learning models and deploy them \emph{locally}~\cite{prun_quanti, pruning_han, quantize, quantize_3bit, quantization, structured_pruning}, i.e., running an alternative ``smaller version'' of a computer vision model at the user-side, to avoid the image upload so that to improve the inference efficiency. Other studies proposed to run a part of a deep learning model \emph{locally}~\cite{ILP_Decoupling, jalad, Edge_LBP, Neurosurgeon}, by decoupling the deep learning model into different parts, e.g., based on the layers in the deep learning model, so that a part of the inference is done \emph{locally} to save some execution time. However, these solutions usually need to re-train the model using the original dataset of the model, which is not practical for today's online computer vision-based services that are merely a black box to end-users, e.g., in the form of a RESTful API.

\subsection{Data Compression}

Data compression solutions study how to compress the original data (e.g., a video or image) to be inferred by the cloud-based deep learning model so that less traffic is used to upload the data to improve inference speed. In recent years, researchers found that conventional human visually optimized-based data compression solutions (e.g., JPEG~\cite{jpeg}, WebP~\cite{calore2010meet} and JPEG2000~\cite{rabbani2002jpeg2000}, etc.) and some recent neural network-based compression solutions~\cite{toderici2017full, theis2017lossy, toderici2015variable, rippel2017real} are not usually applicable to deep learning vision models. Liu et al.~\cite{DeepN-JPEG} revealed that the conventional JPEG image compression framework is designed for the Human-Visual System, which is not suitable for the deep neural network, leading to the computer vision model's inference performance degradation. Dodge et al.~\cite{dodge2016understanding} further discovered that besides the JPEG compression, four types of quality distortions (blur, noise, contrast, and the JPEG2000 compression~\cite{rabbani2002jpeg2000}) can also affect the inference performance of the deep learning models. Delac et al.~\cite{delac2005effects} observed that, in some cases, a high compression quality level does not always reduce the model inference accuracy. %% \\

Based on these insights, Robert et al.~\cite{torfason2018towards} tried to train the deep neural network from the compressed representations of an auto-encoder. Chao et al.~\cite{chao2011preserving} proposed using variable quantization, which is supported by the JPEG standard extension syntax~\cite{dis199110918} to compress the macroblocks in images. Furthermore, they~\cite{chao2013design} designed a quantization table based on the observed impact of scale-space processing on the discrete cosine transform (DCT) basis functions for JPEG images, achieving similar inference performance while reducing the image size overhead effectively. Liu et al.~\cite{DeepN-JPEG} proposed DeepN-JPEG that re-designs the quantization table by linking statistical information of defined features and defined quantization values so that the compressed image size is reduced for deep learning models. Chamain et al.~\cite{2019quannet} proposed a joint optimization of image classification network coupled with the image quantization, achieving image size reduction of JPGE2000~\cite{rabbani2002jpeg2000} encoded images. Recently, Lionel et al.~\cite{gueguen2018faster} presented a new type of neural network that infers directly from the discrete cosine transform coefficients in the middle of the JPEG codec. Baluja et al.~\cite{baluja2019task} proposed task-specific compression that compresses images based on the end-use of images. %% \\

However, such proposals all need one to understand the characteristics of the cloud-based deep learning model and have access to the original training dataset,  \emph{generating} the appropriate compression schemes. To the best of our knowledge, we are the first to propose an adaptive compression configuration solution that learns the optimal compression quality level to achieve an expected inference accuracy and upload image size, only from the online inference results, without knowing details of the model structures. 

In this journal extension, we improve the previous \emph{inference-estimate-retrain} mechanism to cut down the upload image size overhead effectively, add DeepN-JPEG comparative experiment, and amend the manuscript significantly. Especially in the DeepN-JPEG comparative experiment, since Liu et al.~\cite{DeepN-JPEG} evaluated the DeepN-JPEG framework on ImageNet by using four state-of-the-art DNN models (AlexNet~\cite{AlexNet-krizhevsky2012imagenet}, VGG~\cite{VGG-simonyan2014very}, GoogLeNet~\cite{GoogleNet-szegedy2015going} and ResNet~\cite{ResNet-he2016deep}) on local edge devices, which are different from online computer vision-based services, for comparison purpose, we implement the DeepN-JPEG framework according to their paper and evaluate the size reduction and accuracy performance using ImageNet and the metrics in Subsection~\ref{subsec:metrics} on three cloud-based deep learning services (Amazon Rekognition, Face++ and Baidu Vision).

%Liu et al.~\cite{DeepN-JPEG} developed the DeepN-JPEG framework, a deep %neural network favorable JPEG-based image compression framework.